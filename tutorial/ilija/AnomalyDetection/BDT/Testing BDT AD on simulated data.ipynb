{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Boosted Decision Tree based Anomaly Detection on simulated data\n",
    "\n",
    "#### This code generates large dataframe containing multiple timeseries, randomly adds changes in both mean and variance (anomalies), tries to train a BDT to distinguish measurements belonging to the timebin under investigation from measurements in a reference time period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "from pandas.tseries.offsets import *\n",
    "\n",
    "from graphviz import Source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### parameters to set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "np.random.seed(17)\n",
    "\n",
    "n_series = 6\n",
    "start_date = '2017-08-01 00:00:00'\n",
    "end_date = '2017-08-07 23:59:59'\n",
    "\n",
    "# regular behaviour\n",
    "max_noise_amplitude = 0.05 # all the timeseries will have values between 0 and 1\n",
    "\n",
    "# anomalies\n",
    "p_anomaly = 10E-6\n",
    "max_anomaly_duration = 4*3600 # 4 h\n",
    "\n",
    "# tuning parameters\n",
    "cut = 0.55\n",
    "window = 24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### generate normal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dti=pd.DatetimeIndex(start=start_date,end=end_date, freq='s')\n",
    "n_timesteps = len(dti)\n",
    "df = pd.DataFrame()\n",
    "for s in range(n_series):\n",
    "    v = np.random.normal(random.random()/2, max_noise_amplitude/random.randint(1, 8), n_timesteps) \n",
    "    df['link '+str(s)] = pd.Series(v)\n",
    "df['Flag']=0\n",
    "df['auc_score']=0.5\n",
    "df.index = dti\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### generate anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "to_generate = int(n_timesteps * p_anomaly)\n",
    "for a in range(to_generate):\n",
    "    affects = random.sample(range(n_series), random.randint(1, n_series))\n",
    "    duration = int(max_anomaly_duration * random.random())\n",
    "    start = int(n_timesteps * random.random())\n",
    "    end = min(start+duration, n_timesteps)\n",
    "    print('affected:', affects, df.iloc[start].name, df.iloc[end].name)\n",
    "    for s in affects:\n",
    "        df.iloc[start:end,s] = df.iloc[start:end,s] + random.random() * 0.2\n",
    "    df.iloc[start:end,n_series]=1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### enforce range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df[df<0] = 0\n",
    "df[df>1] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### plot timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.plot(figsize=(20,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_for_anomaly(ref, sub):\n",
    "    \n",
    "    y_ref = pd.Series([0] * ref.shape[0])\n",
    "    X_ref = ref\n",
    "    del X_ref['Flag']\n",
    "    del X_ref['auc_score']\n",
    "    \n",
    "    y_sub = pd.Series([1] * sub.shape[0])\n",
    "    X_sub=sub\n",
    "    del X_sub['Flag']\n",
    "    del X_sub['auc_score']\n",
    "    \n",
    "    # separate Reference and Subject into Train and Test\n",
    "    X_ref_train, X_ref_test, y_ref_train, y_ref_test = train_test_split(X_ref, y_ref, test_size=0.3, random_state=42)\n",
    "    X_sub_train, X_sub_test, y_sub_train, y_sub_test = train_test_split(X_sub, y_sub, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # combine training ref and sub samples\n",
    "    X_train = pd.concat([X_ref_train, X_sub_train])\n",
    "    y_train = pd.concat([y_ref_train, y_sub_train])\n",
    "\n",
    "    # combine testing ref and sub samples\n",
    "    X_test = pd.concat([X_ref_test, X_sub_test])\n",
    "    y_test = pd.concat([y_ref_test, y_sub_test])\n",
    "    \n",
    "    clf = AdaBoostClassifier() #dtc\n",
    "#     clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),algorithm=\"SAMME\",n_estimators=200)\n",
    "    \n",
    "    #train an AdaBoost model to be able to tell the difference between the reference and subject data\n",
    "    clf.fit(X_train, y_train) \n",
    "\n",
    "    #Predict using the combined test data\n",
    "    y_predict = clf.predict(X_test)\n",
    "    \n",
    "    # scores = cross_val_score(clf, X, y)\n",
    "    # print(scores)\n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_predict) # calculate the false positive rate and true positive rate\n",
    "    auc_score = auc(fpr, tpr) #calculate the AUC score\n",
    "    print (\"auc_score = \", auc_score, \"\\tfeature importances:\", clf.feature_importances_)\n",
    "    \n",
    "    if auc_score > cut: \n",
    "        plot_roc(fpr, tpr, auc_score)\n",
    "        filename='tree_'+sub.index.min().strftime(\"%Y-%m-%d_%H\")\n",
    "        tree.export_graphviz(clf.estimators_[0] , out_file=filename +'_1.dot') \n",
    "        tree.export_graphviz(clf.estimators_[1] , out_file=filename +'_2.dot') \n",
    "        \n",
    "    return auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_roc(fpr,tpr, roc_auc):\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange', label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', color='r',label='Luck', alpha=.8)\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looping over time intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#find min and max timestamps\n",
    "\n",
    "start = df.index.min()\n",
    "end = df.index.max()\n",
    "\n",
    "#round start \n",
    "start.seconds=0\n",
    "start.minutes=0\n",
    "\n",
    "ref = window * Hour()\n",
    "sub = 1 * Hour()\n",
    "\n",
    "# loop over them\n",
    "ti=start+ref+sub\n",
    "count=0\n",
    "while ti < end + 1 * Minute():\n",
    "    ref_start = ti-ref-sub\n",
    "    ref_end = ti-sub\n",
    "    ref_df = df[(df.index >= ref_start) & (df.index < ref_end)]\n",
    "    sub_df = df[(df.index >= ref_end) & (df.index < ti)]\n",
    "    auc_score = check_for_anomaly(ref_df, sub_df)\n",
    "    df.loc[(df.index>=ref_end) & (df.index<=ti),['auc_score']] = auc_score\n",
    "    print(ti,\"\\trefes:\" , ref_df.shape[0], \"\\tsubjects:\", sub_df.shape[0], '\\tauc:', auc_score)\n",
    "    ti = ti + sub\n",
    "    count=count+1\n",
    "    #if count>2: break\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.plot(figsize=(20,7))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20,7))\n",
    "df.loc[:,'Detected'] = 0\n",
    "df.loc[df.auc_score>0.55,'Detected']=1\n",
    "df.head()\n",
    "ax.plot(df.Flag, 'r')\n",
    "ax.plot(df.auc_score,'g')\n",
    "ax.fill( df.Detected, 'b', alpha=0.3)\n",
    "ax.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
